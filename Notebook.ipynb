{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ul8vTmKlvJl",
        "colab_type": "code",
        "outputId": "35b85302-76f3-4cd2-fad4-4dd0ae50bd0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 30%; color: blue;'><b>Let's Start!</b></marquee>"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='width: 30%; color: blue;'><b>Let's Start!</b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shBAGlXbqATa",
        "colab_type": "text"
      },
      "source": [
        "## **Intro**\n",
        "We have to download the appropriate dataset. Although several sourses provide the specific dataset of 20newsgroups, we follow the instructions from the exercise 6.1.4. There is also the choice to keep or remove some initial meta-information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJs7mubloThM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train_unclear = fetch_20newsgroups(subset='train')\n",
        "newsgroups_train = fetch_20newsgroups (subset='train',remove =('headers', 'footers', 'quotes'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYlAY8ebrofw",
        "colab_type": "text"
      },
      "source": [
        "## Check differences\n",
        "\n",
        "Since both of the two datasets, with or without the meta-information, have the same length, we should continue with the second one, since it is more possible to avoid overfitting. This may happen because the removed information may contain words, or addresses that would cause large learning rates on traning dataset, but smaller ones on testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub5eK6SvqiGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6a842db9-1d6f-4677-aa76-83f40c7b4489"
      },
      "source": [
        "if (newsgroups_train_unclear.filenames.shape) == (newsgroups_train.filenames.shape):\n",
        "  print('Same length')\n",
        "else:\n",
        "  print('Different length')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Same length\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr5vWZ3yuc-L",
        "colab_type": "text"
      },
      "source": [
        "# Present the difference\n",
        "\n",
        "We print the first instance of these two trainign datasets so as to visualize their internal context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgdvLZRXmnOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "e84d4d78-2b7b-4917-cd33-f466edc2c8eb"
      },
      "source": [
        "print(\"With meta-information: \\n\")\n",
        "print(newsgroups_train_unclear.data[0])\n",
        "\n",
        "print(\"Without meta-information: \\n\")\n",
        "print(newsgroups_train.data[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With meta-information: \n",
            "\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Without meta-information: \n",
            "\n",
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylm3N5TSvDTX",
        "colab_type": "text"
      },
      "source": [
        "# Formulation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNHdjhrlusk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "7ca843a0-04a2-4d0c-f75b-796754772645"
      },
      "source": [
        "print(\"Number of training instances: \", newsgroups_train.filenames.shape)\n",
        "print(\"Number of labels: \", len(newsgroups_train.target_names))\n",
        "\n",
        "print('Label names: \\n')\n",
        "for i in set(newsgroups_train.target_names):\n",
        "  print(i)\n",
        "\n",
        "print('Label ids inside the dataset: \\n')\n",
        "for i in set(newsgroups_train.target):\n",
        "  print(i)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training instances:  (11314,)\n",
            "Number of labels:  20\n",
            "Label names: \n",
            "\n",
            "comp.graphics\n",
            "comp.windows.x\n",
            "rec.autos\n",
            "comp.sys.mac.hardware\n",
            "sci.electronics\n",
            "soc.religion.christian\n",
            "rec.sport.baseball\n",
            "talk.politics.misc\n",
            "comp.os.ms-windows.misc\n",
            "comp.sys.ibm.pc.hardware\n",
            "sci.space\n",
            "talk.politics.mideast\n",
            "misc.forsale\n",
            "rec.sport.hockey\n",
            "sci.crypt\n",
            "talk.politics.guns\n",
            "rec.motorcycles\n",
            "alt.atheism\n",
            "talk.religion.misc\n",
            "sci.med\n",
            "Label ids inside the dataset: \n",
            "\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUhZn22Ox8S6",
        "colab_type": "text"
      },
      "source": [
        "### Download some packages for text-processing.\n",
        "\n",
        "#### We moreover set some default stopwords for more experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tytCM7ulx7Vu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0d9c3242-7225-40d3-a713-720404dce060"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_our = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
        " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
        " 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
        " 'each', 'few', 'for', 'from', 'further', \n",
        " 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
        " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
        " 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
        " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
        " 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
        " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
        " 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
        " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
        " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
        " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
        " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
        " 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
        " '4th', '5th', '6th', '7th', '8th', '9th', '10th']"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feaiDl9LyOSV",
        "colab_type": "text"
      },
      "source": [
        "#### We define a typical Tokenizer so as to combine the next actions:\n",
        "\n",
        "\n",
        "1.   Lower all characters\n",
        "2.   Tokenize each text segment, so as to obtain all the words\n",
        "3.   Remove some usual stopwords, for having a more clear version of each segment\n",
        "4.   Stem the remaining words, mapping some similar words as the same ones (e.g. faded or fading -> fade)\n",
        "5.   Remove words with length smaller than 3 characters\n",
        "\n",
        "--->>Two variants have been build here for being examined later!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZH4DxtwmPTI",
        "colab_type": "code",
        "outputId": "83f199ee-ca23-435a-bcad-d30737edc90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def Tokenizer1(str_input, remove_stops = True, apply_stem = True, remove_short = True):\n",
        "    str_input = str_input.lower()\n",
        "    words = word_tokenize(str_input)\n",
        "\n",
        "    #we try to remove words with only 2 characters\n",
        "    if remove_short:\n",
        "      words = [word for word in words if len(word) > 2]\n",
        "    \n",
        "    #remove stopwords\n",
        "    if remove_stops:\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    #stem the words\n",
        "    if apply_stem:\n",
        "      porter_stemmer=nltk.PorterStemmer()\n",
        "      words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "def Tokenizer2(str_input, remove_stops = False, apply_stem = False, remove_short = False):\n",
        "    str_input = str_input.lower()\n",
        "    words = word_tokenize(str_input)\n",
        "    \n",
        "    #we try to remove words with only 2 characters\n",
        "    if remove_short:\n",
        "      words = [word for word in words if len(word) > 2]\n",
        "    \n",
        "    #remove stopwords\n",
        "    if remove_stops:\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      words = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    #stem the words\n",
        "    if apply_stem:\n",
        "      porter_stemmer=nltk.PorterStemmer()\n",
        "      words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "    return words\n",
        "\n",
        "print('Example of using or not each component of pre=processing into the Tokenizer')\n",
        "print(Tokenizer2(newsgroups_train.data[15]))\n",
        "#print(Tokenizer(newsgroups_train.data[15], True,  False))\n",
        "#print(Tokenizer(newsgroups_train.data[15], False, True ))\n",
        "print(Tokenizer1(newsgroups_train.data[15]))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of using or not each component of pre=processing into the Tokenizer\n",
            "['it', 'was', 'a', 'test', 'of', 'the', 'first', 'reusable', 'tool', '.', 'pointy', 'so', 'they', 'can', 'find', 'them', 'or', 'so', 'they', 'will', 'stick', 'into', 'their', 'pants', 'better', ',', 'and', 'be', 'closer', 'to', 'their', 'brains', '?']\n",
            "['test', 'first', 'reusabl', 'tool', 'pointi', 'find', 'stick', 'pant', 'better', 'closer', 'brain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53pdcRlRGSc-",
        "colab_type": "text"
      },
      "source": [
        "## Examine a partial version of the total dataset\n",
        "\n",
        "We employ the tfidf transformation for examining the efficacy over 4 different classes of the total training corpus with 2 variants of our Tokenizer. Thus, we insert a pipeline structure which contains the tfidf Vectorizer along with 3 selected classifiers:\n",
        "\n",
        "1.   SGD, a classifier based on linear models\n",
        "2.   Multinomial NB, a well-known classifier for textual data\n",
        "3.   GradientBoosting (GNB), a powerful predictive algorithm that is based on several weak estimators trying to catch the errors of previous iterations so as to learn the underlying problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84m6CMyZ3IMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier as GB\n",
        "from sklearn.naive_bayes import MultinomialNB  as MNB\n",
        "from sklearn.linear_model import SGDClassifier as SGD\n",
        "\n",
        "\n",
        "text_clf1 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
        "    ('clf',   SGD(max_iter=1000)),\n",
        "])\n",
        "\n",
        "\n",
        "text_clf2 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer2, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
        "    ('clf',   SGD(max_iter=1000)),\n",
        "])\n",
        "\n",
        "text_clf3 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
        "    ('clf',   MNB()),\n",
        "])\n",
        "\n",
        "\n",
        "text_clf4 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer2, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
        "    ('clf',   MNB()),\n",
        "])\n",
        "\n",
        "text_clf5 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
        "    ('clf',   GB()),\n",
        "])\n",
        "\n",
        "\n",
        "text_clf6 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer2, max_df=0.3, min_df=0.0002, max_features=1000)),\n",
        "    ('clf',   GB()),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFbWVy5dKxuV",
        "colab_type": "text"
      },
      "source": [
        "## Download the reduced training dataset along with its corresponding test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-qf1_JKw05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af4f558b-6ffc-4e42-ae81-708b69e41be1"
      },
      "source": [
        "categories = ['alt.atheism', 'talk.religion.misc',\n",
        "              'comp.graphics', 'sci.space']\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups (subset='train',remove = ('headers', 'footers', 'quotes'), categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups (subset='test',  remove = ('headers', 'footers', 'quotes'), categories=categories)\n",
        "\n",
        "print('Size of train data: ', newsgroups_train.filenames.shape)\n",
        "print('Size of test  data: ', newsgroups_test.filenames.shape)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train data:  (2034,)\n",
            "Size of test  data:  (1353,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1mdfvO2LepU",
        "colab_type": "text"
      },
      "source": [
        "### Apply both of the examined variants on train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naTplv9b4SR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf1.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train1 = text_clf1.predict(newsgroups_train.data)\n",
        "predictions_test1 = text_clf1.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "text_clf2.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train2 = text_clf2.predict(newsgroups_train.data)\n",
        "predictions_test2 = text_clf2.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "text_clf3.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train3 = text_clf3.predict(newsgroups_train.data)\n",
        "predictions_test3 = text_clf3.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "text_clf4.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train4 = text_clf4.predict(newsgroups_train.data)\n",
        "predictions_test4 = text_clf4.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "text_clf5.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train5 = text_clf5.predict(newsgroups_train.data)\n",
        "predictions_test5 = text_clf5.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "text_clf6.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train6 = text_clf6.predict(newsgroups_train.data)\n",
        "predictions_test6 = text_clf6.predict(newsgroups_test.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ZSnar4Lpkz",
        "colab_type": "text"
      },
      "source": [
        "#### We can have a first sample of the predictions vs the reality:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKb3eFH85H0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bdc078bf-e6ee-474e-9933-88edcd6e42f8"
      },
      "source": [
        "print(predictions_train1[0:20])\n",
        "print(predictions_train2[0:20])\n",
        "print(newsgroups_train.target[0:20])\n",
        "\n",
        "print(predictions_test1[0:20])\n",
        "print(predictions_test2[0:20])\n",
        "print(newsgroups_test.target[0:20])\n"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 3 2 3 2 0 2 1 2 1 2 1 0 2 1 2 0 2 2 3]\n",
            "[1 3 2 3 2 0 2 1 2 1 2 1 2 2 1 2 0 2 2 3]\n",
            "[1 3 2 0 2 0 2 1 2 1 2 1 1 2 1 2 0 2 2 3]\n",
            "[2 1 1 1 1 1 2 2 0 0 1 1 1 2 1 0 3 0 1 2]\n",
            "[2 1 1 1 1 1 2 2 0 2 1 1 1 2 1 1 3 3 1 2]\n",
            "[2 1 1 1 1 1 2 2 0 2 1 1 1 2 1 0 0 0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWlmtvXXQkl2",
        "colab_type": "text"
      },
      "source": [
        "#### Or we could also obtain the final transformation of the input data based on their tfidf format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13zPCqU86SJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c4d3a816-5c13-4980-f65a-d623d60bfefa"
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=Tokenizer1, max_df=0.3, min_df=0.0002, max_features=1000)\n",
        "\n",
        "X_tfidf_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "print(X_tfidf_train.shape)\n",
        "\n",
        "X_tfidf_test = vectorizer.fit_transform(newsgroups_test.data)\n",
        "print(X_tfidf_test.shape)"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2034, 1000)\n",
            "(1353, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2RHAU2gMJWW",
        "colab_type": "text"
      },
      "source": [
        "## But we should prefer to obtain the overall behavior of the examined results:\n",
        "\n",
        "**On all cases we see that the first variant of Tokenizer is the best. Thus we hold its combination with the use of MNB.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r8dhIk46g3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "720e6f55-7aa9-4f97-800e-71cd47cb5353"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test1))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test1, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test1))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test1))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test2))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test2, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test2))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test2))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test3))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test3, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test3))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test3))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test4))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test4, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test4))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test4))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test5))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test5, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test5))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test5))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test6))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test6, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test6))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test6))"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.679970436067997\n",
            "Precision: 0.6834689748419724\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.60      0.57       319\n",
            "           1       0.83      0.81      0.82       389\n",
            "           2       0.75      0.74      0.74       394\n",
            "           3       0.52      0.50      0.51       251\n",
            "\n",
            "    accuracy                           0.68      1353\n",
            "   macro avg       0.66      0.66      0.66      1353\n",
            "weighted avg       0.68      0.68      0.68      1353\n",
            "\n",
            "[[190  14  31  84]\n",
            " [ 26 314  37  12]\n",
            " [ 50  33 290  21]\n",
            " [ 81  17  27 126]]\n",
            "Accuracy: 0.6548410938654841\n",
            "Precision: 0.6602623338088932\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.52      0.54       319\n",
            "           1       0.78      0.79      0.79       389\n",
            "           2       0.74      0.70      0.72       394\n",
            "           3       0.46      0.55      0.50       251\n",
            "\n",
            "    accuracy                           0.65      1353\n",
            "   macro avg       0.64      0.64      0.64      1353\n",
            "weighted avg       0.66      0.65      0.66      1353\n",
            "\n",
            "[[165  20  43  91]\n",
            " [ 18 309  36  26]\n",
            " [ 38  40 275  41]\n",
            " [ 72  25  17 137]]\n",
            "Accuracy: 0.729490022172949\n",
            "Precision: 0.7283138502518092\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.69      0.64       319\n",
            "           1       0.83      0.87      0.85       389\n",
            "           2       0.75      0.85      0.79       394\n",
            "           3       0.69      0.39      0.50       251\n",
            "\n",
            "    accuracy                           0.73      1353\n",
            "   macro avg       0.72      0.70      0.70      1353\n",
            "weighted avg       0.73      0.73      0.72      1353\n",
            "\n",
            "[[219  16  42  42]\n",
            " [ 14 337  36   2]\n",
            " [ 24  37 333   0]\n",
            " [104  14  35  98]]\n",
            "Accuracy: 0.696969696969697\n",
            "Precision: 0.6991155382373906\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.60       319\n",
            "           1       0.81      0.84      0.82       389\n",
            "           2       0.72      0.82      0.77       394\n",
            "           3       0.68      0.33      0.44       251\n",
            "\n",
            "    accuracy                           0.70      1353\n",
            "   macro avg       0.69      0.66      0.66      1353\n",
            "weighted avg       0.70      0.70      0.68      1353\n",
            "\n",
            "[[211  23  52  33]\n",
            " [ 18 326  43   2]\n",
            " [ 34  32 324   4]\n",
            " [116  23  30  82]]\n",
            "Accuracy: 0.7014042867701404\n",
            "Precision: 0.6987983922155204\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.61      0.61       319\n",
            "           1       0.82      0.81      0.81       389\n",
            "           2       0.69      0.82      0.75       394\n",
            "           3       0.62      0.47      0.54       251\n",
            "\n",
            "    accuracy                           0.70      1353\n",
            "   macro avg       0.69      0.68      0.68      1353\n",
            "weighted avg       0.70      0.70      0.70      1353\n",
            "\n",
            "[[194  13  56  56]\n",
            " [ 18 314  51   6]\n",
            " [ 26  36 322  10]\n",
            " [ 76  19  37 119]]\n",
            "Accuracy: 0.6755358462675536\n",
            "Precision: 0.673874977851693\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.58      0.57       319\n",
            "           1       0.83      0.79      0.81       389\n",
            "           2       0.68      0.80      0.74       394\n",
            "           3       0.57      0.43      0.49       251\n",
            "\n",
            "    accuracy                           0.68      1353\n",
            "   macro avg       0.66      0.65      0.65      1353\n",
            "weighted avg       0.67      0.68      0.67      1353\n",
            "\n",
            "[[184  18  51  66]\n",
            " [ 24 306  53   6]\n",
            " [ 38  30 316  10]\n",
            " [ 84  14  45 108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2HgQNoxqHW4",
        "colab_type": "text"
      },
      "source": [
        "## Use of n-grams\n",
        "\n",
        "#### For further analysis, we proceed by examining 3 variants of the best combination:\n",
        "\n",
        "\n",
        "\n",
        "1.   Default arguments of TfidfVectorizer\n",
        "2.   Use of unigrams and bigrams\n",
        "3.   Use of bigrams only\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztzr1HVvosf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_clf3_variant1 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1)),\n",
        "    ('clf',   MNB()),\n",
        "])\n",
        "\n",
        "text_clf3_variant2 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, ngram_range = (1,2))),\n",
        "    ('clf',   MNB()),\n",
        "])\n",
        "\n",
        "\n",
        "text_clf3_variant3 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, ngram_range = (2,2))),\n",
        "    ('clf',   MNB()),\n",
        "])\n",
        "\n",
        "\n",
        "text_clf3_variant1.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train1 = text_clf3_variant1.predict(newsgroups_train.data)\n",
        "predictions_test1 = text_clf3_variant1.predict(newsgroups_test.data)\n",
        "\n",
        "text_clf3_variant2.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train2 = text_clf3_variant2.predict(newsgroups_train.data)\n",
        "predictions_test2 = text_clf3_variant2.predict(newsgroups_test.data)\n",
        "\n",
        "text_clf3_variant3.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train3 = text_clf3_variant3.predict(newsgroups_train.data)\n",
        "predictions_test3 = text_clf3_variant3.predict(newsgroups_test.data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1SACWCXqqxW",
        "colab_type": "text"
      },
      "source": [
        "## Results: We observe the better generalization ability over test data when we use both unigrams and bigrams!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8N4Ti1nqsTQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "2a9b9e9a-fe24-44de-ae9f-86226baa7cde"
      },
      "source": [
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test1))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test1, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test1))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test1))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test2))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test2, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test2))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test2))\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test3))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test3, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test3))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test3))\n"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7287509238728751\n",
            "Precision: 0.74671267456911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.71      0.65       319\n",
            "           1       0.88      0.89      0.89       389\n",
            "           2       0.71      0.91      0.80       394\n",
            "           3       0.80      0.21      0.33       251\n",
            "\n",
            "    accuracy                           0.73      1353\n",
            "   macro avg       0.74      0.68      0.67      1353\n",
            "weighted avg       0.75      0.73      0.70      1353\n",
            "\n",
            "[[227  14  65  13]\n",
            " [  4 347  38   0]\n",
            " [ 14  21 359   0]\n",
            " [139  13  46  53]]\n",
            "Accuracy: 0.7191426459719142\n",
            "Precision: 0.7478169629923236\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.71      0.65       319\n",
            "           1       0.89      0.88      0.88       389\n",
            "           2       0.67      0.93      0.77       394\n",
            "           3       0.84      0.17      0.28       251\n",
            "\n",
            "    accuracy                           0.72      1353\n",
            "   macro avg       0.75      0.67      0.65      1353\n",
            "weighted avg       0.75      0.72      0.68      1353\n",
            "\n",
            "[[225  15  71   8]\n",
            " [  3 341  45   0]\n",
            " [  9  20 365   0]\n",
            " [133   9  67  42]]\n",
            "Accuracy: 0.6104951958610495\n",
            "Precision: 0.6420093765021876\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.45      0.52       319\n",
            "           1       0.68      0.83      0.75       389\n",
            "           2       0.55      0.85      0.66       394\n",
            "           3       0.76      0.10      0.18       251\n",
            "\n",
            "    accuracy                           0.61      1353\n",
            "   macro avg       0.65      0.56      0.53      1353\n",
            "weighted avg       0.64      0.61      0.56      1353\n",
            "\n",
            "[[144  49 118   8]\n",
            " [  5 323  61   0]\n",
            " [  8  53 333   0]\n",
            " [ 75  51  99  26]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hSay3awsEon",
        "colab_type": "text"
      },
      "source": [
        "## Tuning stage\n",
        "\n",
        "#### We apply a grid search classifier trying to find better parameters for the whole pipeline. More experiments can be selected, here we have created a grid of 24 separate cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hk2FpgPer07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a0548c5e-6ced-47b8-897e-2b97277f8e9b"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {\n",
        "    'tfidf__max_features': (5000, None),\n",
        "    'tfidf__max_df': (1, 0.75, 0.5),\n",
        "    'tfidf__norm': ('l1', 'l2'),\n",
        "    'clf__alpha': (0.9, 1),\n",
        "    #'clf__n_estimators': (50, 100, 250),\n",
        "}\n",
        "\n",
        "text_tune = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer1, ngram_range = (1,2))),\n",
        "    ('clf',   MNB()),\n",
        "])\n",
        "\n",
        "\n",
        "gs_clf = GridSearchCV(text_tune, parameters, cv=3, iid=False, n_jobs=-1)\n",
        "gs_clf.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "print(\"Best score: %0.3f\" % gs_clf.best_score_)\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = gs_clf.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best score: 0.792\n",
            "Best parameters set:\n",
            "\tclf__alpha: 0.9\n",
            "\ttfidf__max_df: 0.75\n",
            "\ttfidf__max_features: 5000\n",
            "\ttfidf__norm: 'l2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYVbSIou3CuY",
        "colab_type": "text"
      },
      "source": [
        "## We apply the best parameters and indeed obtain better predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsRXYk_fXlju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a55b48da-a05a-4e07-e0f6-a5abd301e97b"
      },
      "source": [
        "text_clf_tuned = text_tune.set_params(**best_parameters)\n",
        "text_clf_tuned.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train = text_clf_tuned.predict(newsgroups_train.data)\n",
        "predictions_test = text_clf_tuned.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test))\n"
      ],
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7590539541759054\n",
            "Precision: 0.7646233573261035\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.72      0.67       319\n",
            "           1       0.88      0.90      0.89       389\n",
            "           2       0.77      0.88      0.82       394\n",
            "           3       0.77      0.39      0.52       251\n",
            "\n",
            "    accuracy                           0.76      1353\n",
            "   macro avg       0.76      0.72      0.72      1353\n",
            "weighted avg       0.76      0.76      0.75      1353\n",
            "\n",
            "[[231  13  47  28]\n",
            " [  8 351  29   1]\n",
            " [ 22  25 347   0]\n",
            " [113  11  29  98]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRoWs_Ne3Sl-",
        "colab_type": "text"
      },
      "source": [
        "# Let's examine the total dataset with the best learner obtained by a sample of the total data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzFtkvvqzxJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c401b021-6a76-4ae8-d157-52c18546de0b"
      },
      "source": [
        "newsgroups_train = fetch_20newsgroups (subset='train',remove = ('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups (subset='test',  remove = ('headers', 'footers', 'quotes'))\n",
        "\n",
        "\n",
        "text_clf_tuned.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "predictions_train2 = text_clf_tuned.predict(newsgroups_train.data)\n",
        "predictions_test2 = text_clf_tuned.predict(newsgroups_test.data)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions_test2))\n",
        "print(\"Precision:\", precision_score(newsgroups_test.target, predictions_test2, average='weighted'))\n",
        "print(classification_report(newsgroups_test.target, predictions_test2))\n",
        "print(confusion_matrix(newsgroups_test.target, predictions_test2))\n"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6477695167286245\n",
            "Precision: 0.6699598130689187\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.32      0.40       319\n",
            "           1       0.54      0.64      0.58       389\n",
            "           2       0.61      0.55      0.58       394\n",
            "           3       0.58      0.63      0.60       392\n",
            "           4       0.66      0.58      0.62       385\n",
            "           5       0.71      0.73      0.72       395\n",
            "           6       0.78      0.74      0.76       390\n",
            "           7       0.70      0.67      0.68       396\n",
            "           8       0.75      0.70      0.73       398\n",
            "           9       0.86      0.77      0.81       397\n",
            "          10       0.55      0.91      0.69       399\n",
            "          11       0.72      0.72      0.72       396\n",
            "          12       0.61      0.50      0.55       393\n",
            "          13       0.78      0.69      0.73       396\n",
            "          14       0.78      0.72      0.75       394\n",
            "          15       0.45      0.88      0.59       398\n",
            "          16       0.56      0.73      0.63       364\n",
            "          17       0.79      0.75      0.77       376\n",
            "          18       0.67      0.31      0.42       310\n",
            "          19       0.79      0.06      0.11       251\n",
            "\n",
            "    accuracy                           0.65      7532\n",
            "   macro avg       0.67      0.63      0.62      7532\n",
            "weighted avg       0.67      0.65      0.64      7532\n",
            "\n",
            "[[101   4   1   0   0   3   0   4   6   5  10   5   1   8   8 132  13  14\n",
            "    2   2]\n",
            " [  3 248  22  13  12  37   5   2   5   1   7  10   6   2   9   5   1   1\n",
            "    0   0]\n",
            " [  2  31 217  41  15  33   4   2   5   2  18   6   2   5   5   4   0   1\n",
            "    1   0]\n",
            " [  0   9  37 248  33   9  10   5   0   0   7   7  24   0   1   0   2   0\n",
            "    0   0]\n",
            " [  0  13  25  47 223   6  18   5   0   1  15   7  16   2   3   3   1   0\n",
            "    0   0]\n",
            " [  0  45  13   5   8 289   6   4   0   2   8   6   2   0   2   4   1   0\n",
            "    0   0]\n",
            " [  0   2   3  32  14   1 288  12   6   4  11   2   6   2   3   2   0   1\n",
            "    1   0]\n",
            " [  2   3   3   2   2   4   9 266  24   5  31   3  16   4   7   4   4   4\n",
            "    3   0]\n",
            " [  0   3   0   1   3   1   5  30 280   9  21   3  12   5   1  13   4   3\n",
            "    4   0]\n",
            " [  2   8   1   1   2   5   2   3   3 307  35   2   1   6   0  13   3   2\n",
            "    1   0]\n",
            " [  1   2   0   1   0   1   0   2   1   5 365   5   2   1   1   8   3   0\n",
            "    1   0]\n",
            " [  1  10   8   3   5   5   4   4   1   1  20 285   4   3   8   7  19   6\n",
            "    2   0]\n",
            " [  2  30  15  31  17   4  11  12  13   1  12  23 196   8   8   4   3   3\n",
            "    0   0]\n",
            " [  4  17   2   1   2   3   5  11   8   4  20   1  10 272   5  17   7   4\n",
            "    3   0]\n",
            " [  2  14   4   0   0   2   0   8   5   1  21   5  14   9 283  15   4   2\n",
            "    5   0]\n",
            " [  8   8   1   0   0   1   2   0   0   1  14   2   1   4   2 350   2   1\n",
            "    1   0]\n",
            " [  5   2   2   0   0   1   1   4   5   0  17  11   2   7   3  20 267   9\n",
            "    7   1]\n",
            " [ 14   5   1   1   0   1   0   1   6   3   7   3   2   2   0  29   6 283\n",
            "   12   0]\n",
            " [ 13   1   0   0   1   2   0   3   5   2  11   8   4   4  11  28 104  16\n",
            "   96   1]\n",
            " [ 27   4   2   1   0   1   1   3   1   4  11   2   1   4   5 125  33   7\n",
            "    4  15]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBLaCC2zt5K",
        "colab_type": "code",
        "outputId": "467584df-c20e-42bd-bc6b-effc7a6271e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "source": [
        "%%html\n",
        "<marquee style='width: 30%; color: blue;'><b>Finish!</b></marquee>"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<marquee style='width: 30%; color: blue;'><b>Finish!</b></marquee>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}